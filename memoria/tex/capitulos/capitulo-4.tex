%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Capítulo 4: Relación entre los PO y los PNM           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

En el capítulo anterior hemos conocido a los procesos de Markov discretos, más frecuentemente denominados \textit{cadenas de Markov}. Por otra parte, en los dos primeros capítulos hemos visto la teoría fundamental de polinomios ortogonales, profundizando en los conocidos polinomios clásicos. Aparentemente, son dos ámbitos completamente diferentes: los polinomios ortogonales están muy relacionados con teoría de la medida, álgebra lineal y análisis funcional, mientras que las cadenas de Markov son modelos meramente probabilísticos. A pesar de ello, ambas disciplinas confluyen en un caso particular de cadena de Markov: los procesos de nacimiento y muerte (\textit{birth-and-death processes}). 

Los procesos de nacimiento y muerte discretos, también llamados cadenas de nacimiento y muerte, caminatas aleatorias, o en inglés más conocidos como \textit{random walks}, son cadenas de Markov en la que desde un determinado estado $i$ tan solo es posible acceder al siguiente ($i+1$), al anterior ($i-1$), o permanecer en el mismo estado ($i$). Fueron los matemáticos Samuel Karlin (1924--2007) y James McGregor (1921--1988) quienes profundizaron en la relación de estas particulares cadenas con los polinomios ortogonales, destacando la que llamaremos fórmula de representación de Karlin y McGregor.

Para la redacción de este capítulo se han usado como bibliografía fundamental las referencias \cite[Ch. II]{Manuel}, \cite[Ch. II]{schoutens-2000}, \cite{random-walks}, \cite{Linear-Growth}, entre otras.

\section{Procesos de nacimiento y muerte}

Como mencionamos anteriormente, un proceso de nacimiento y muerte discreto, cadena de nacimiento y muerte o caminata aleatoria es un caso particular de proceso de Markov en el que la única transición posible desde un estado $i\in S$ es hacia el estado siguiente, $i+1$, hacia el anterior, $i-1$, o hacia el propio estado $i$. Intuitivamente, es como un juego de plataformas infinito. Entonces, tomamos $S=\N_0 = \{0,1,2,\dots\}$\footnote{Existe una extensión en la que $S = \{-1,0,1,2,\dots\}$ y el estado $-1$ es un estado absorbente. Nosotros supondremos $S=\N_0$, aunque si se desea se puede consultar \cite[Ch. II, Section 2.3]{schoutens-2000} para más información.} y, por esta suposición, tenemos la matriz tridiagonal infinita

\begin{equation}
    \label{eq:matriz-pnm}
    P = \begin{pmatrix}
        r_0 & p_0 & 0 & 0 & \cdots \\
        q_1 & r_1 & p_1 & 0 & \cdots \\
        0 & q_2 & r_2 & p_2 & \cdots \\
        \vdots & \ddots & \ddots & \ddots & \ddots 
    \end{pmatrix},
\end{equation}
donde
\begin{itemize}
    \item $q_i = P[X_{n+1}=i-1|X_n=i]$, $i=1,2,\dots$
    \item $r_i = P[X_{n+1}=i|X_n=i]$, $i=0,1,2,\dots$
    \item $p_i = P[X_{n+1}=i+1|X_n=i]$, $i=0,1,2,\dots$
    \item Asumimos que $q_i > 0 \ (i\geq 1)$, $r_i\geq 0 \  (i\geq 0)$, $p_i>0 \ (i\geq 0)$, $r_0+p_0 =1$, $q_i+r_i+p_i=1 \ (i\geq 1)$.
\end{itemize}

Es decir, siempre es posible avanzar o retroceder, permitiendo que no siempre sea posible mantenerse en el mismo estado. En la imagen \ref{img:diagrama-trans-pnm} se representa el diagrama de estados asociado a esta particular cadena.

\begin{figure}[ht] 
    \label{img:diagrama-trans-pnm}
    \centering 
      \begin{tikzpicture}[->, %Líneas direccionadas
                         >=stealth', %Puntas de flecha rellenas
                         shorten >=1pt,
                         auto,
                         node distance=2cm,] %distancia mínima entre nodos
    
          
       \node[state] (q0) {$0$};
       \node[state, right of=q0] (q1) {$1$};
       \node[state, right of=q1] (q2) {$2$};
       \node[right of=q2] (q3) {$\dots$};
       
       \draw (q0) edge[loop above] node{$r_0$} (q0);
       \draw (q0) edge[bend left, above] node{$p_0$} (q1);
       \draw (q1) edge[bend left, below] node{$q_1$} (q0);
       \draw (q1) edge[loop above] node{$r_1$} (q1);
       \draw (q1) edge[bend left, above] node{$p_1$} (q2);
       \draw (q2) edge[bend left, below] node{$q_2$} (q1);
       \draw (q2) edge[loop above] node{$r_2$} (q2);
       \draw (q2) edge[bend left, above] node{$p_2$} (q3);
       \draw (q3) edge[bend left, below] node{$q_3$} (q2);
       
      \end{tikzpicture}
    \caption{Diagrama de transición de un proceso de nacimiento y muerte}
    \end{figure}

    El comportamiento a largo plazo de los procesos de nacimiento y muerte puede explicarse a partir de unos coeficientes que definimos a continuación:

    \begin{definicion}[Coeficientes potenciales]
        Dado un proceso de nacimiento y muerte $\{X_n:n\geq 0\}$ con matriz de transición $P$ definida como en \eqref{eq:matriz-pnm}, se definen sus \textbf{coeficientes potenciales} como
        \begin{align}
            \label{eq:coeficientes-potenciales}
            \pi_0&= 1 & \pi_n &= \dfrac{p_0 p_1 \cdots p_{n-1}}{q_1 q_2 \cdots q_n} \ \ \ (n\geq 1)
        \end{align}
    \end{definicion}
    Nótese que, para $n\geq 1$ los coeficientes potenciales cumplen la relación de recurrencia $\pi_n = \pi_{n-1} \dfrac{p_{n-1}}{q_n}$, de la cual podemos deducir las siguientes ecuaciones de simetría:
    \begin{equation}
        \label{eq:ecs-simetria}
        \pi_n p_n = \pi_{n+1}q_{n+1} \ \ \ (n\geq 0).
    \end{equation} 
    Por otro lado, obsérvese que, con las condiciones impuestas sobre $P$, el vector fila infinito $\pi = (\pi_n)_{n\in\N_0}$ verifica $\pi P = \pi$, la ecuación \eqref{eq:balance} extendida a vectores y matrices infinitos. Por tanto, $\pi$ es un vector estacionario de la cadena, que de hecho será una distribución estacionaria si $\sum_{n=0}^\infty \pi_n < \infty$.

    Por la estructura que posee la matriz $P$, podemos comprobar que siempre es irreducible y es aperiódica si existe algún $r_n>0$. Bajo estas hipótesis, podemos pensar que es posible aplicar el teorema \ref{th:teorema-limite} para asegurar la existencia de una distribución límite, pero recordemos que en este caso estamos tratando con cadenas de Markov infinitas. Afortunadamente, en \cite[Theorem 5.5]{Seneta} y en \cite[Theorem 2.3]{Manuel} podemos encontrar generalizaciones del teorema \ref{th:teorema-limite} que nos ayudan a deducir que el vector infinito $\pi$ representa la distribución límite del proceso de nacimiento y muerte.

    
    \section{La fórmula de representación de Karlin-McGregor}
    \label{section:formula-representacion}

    En la última sección hemos presentado las cadenas de nacimiento y muerte desde un punto de vista probabilístico, extendiendo lo aprendido en el capítulo \ref{chap:Markov}. En esta y en las próximas secciones estudiaremos qué relación existe entre estas particulares cadenas y los polinomios ortogonales. Para ello nos ayudaremos de varias técnicas analíticas, entre las que destacamos el conocido teorema espectral (teorema \ref{th:espectral}), presentado con mayor nivel de detalle en el apéndice \ref{appendix:teorema-espectral}.
    
    Recordemos que las cadenas de nacimiento y muerte son cadenas de Markov en las que desde el estado $i$-ésimo tan solo se puede retroceder al estado anterior con probabilidad $q_i$, mantenerse en el mismo con probabilidad $r_i$ o avanzar al siguiente con probabilidad $p_i$, obteniendo una matriz del proceso como la presentada en \eqref{eq:matriz-pnm}.

    A partir de los coeficientes $\{q_n\}_{n\geq 1}$, $\{r_n\}_{n\geq 0}$ y $\{p_n\}_{n\geq 0}$ definimos recurrentemente la sucesión de polinomios $\{Q_n\}$ como

    \begin{equation}
        \label{eq:polinomios-pnm}
        \begin{array}{l}
            Q_{-1}(x) = 0, \ \ \ Q_0(x)=1, \\
            xQ_0(x)=p_0 Q_1(x) + r_0 Q_0(x), \\
            x Q_n(x) = p_n Q_{n+1}(x) + r_n Q_n(x) + q_n Q_{n-1}(x), \ \ \ (n\geq 1).
        \end{array}
    \end{equation}
    
    Nótese la similitud entre estas ecuaciones y la relación de recurrencia a tres términos \eqref{eq:RRTT} de los polinomios ortogonales. 

    Consideramos ahora un espacio de Hilbert concreto, el espacio $\ell^2_\pi(\N_0)$, definido como el espacio de sucesiones complejas tales que 
    $$
    \|f\|_\pi^2=\sum_{n=0}^\infty |f_n|^2\pi_n <\infty,
    $$
    donde los coeficientes $\pi_n$ son los dados en \eqref{eq:coeficientes-potenciales}. Para cuando sea necesario, asumimos que $f_n=0$ siempre que $n<0$.

    Teniendo presente la matriz $P$ dada en \eqref{eq:matriz-pnm}, en el espacio de Hilbert $\ell^2_\pi(\N_0)$, asumiendo un abuso de notación consideramos la transformación
    \begin{equation}
        \label{eq:operador-P}
        \begin{split}
            P : \ell^2_\pi(\N_0) & \longrightarrow \ell^2_\pi(\N_0) \\
            f & \longmapsto Pf,
        \end{split}
    \end{equation}
    donde
    $$
    (Pf)_n = \sum_{m=0}^\infty P_{nm} f_m = p_n f_{n+1} + r_n f_n + q_n f_{n-1}. 
    $$
    Obsérvese que en realidad $Pf$ es el resultado de multiplicar la matriz $P$ por $f$ visto como un vector columna.

    El objetivo es aplicar el teorema espectral al operador $P$, aunque para ello debemos asegurarnos de que verifica sus hipótesis.

    \begin{lema}
        El operador $P$ definido en \eqref{eq:operador-P} es un operador lineal, acotado y autoadjunto sobre el espacio de Hilbert $\ell^2_\pi(\N_0)$ cuya norma verifica $\|P\|\leq 1$.
    \end{lema}
    \begin{proof} Sean $f,g\in\ell^2_\pi(\N_0)$.
        \begin{itemize}
            \item Lineal: Es trivial.
            \item Acotado:
            \begin{equation*}
                \begin{split}
                    \|Pf\|_\pi^2 &= \sum_{n=0}^\infty |p_n f_{n+1} + r_n f_n + q_n f_{n-1}|^2 \pi_n \ \ \ \text{(desigualdad triangular y }p_n,r_n,q_n\leq 1\text{)} \\
                    &\leq \sum_{n=0}^\infty |f_{n-1}|^2\pi_n + \sum_{n=0}^\infty |f_{n}|^2\pi_n + \sum_{n=0}^\infty |f_{n+1}|^2\pi_n \\
                    &=  \sum_{n=0}^\infty \frac{q_n}{p_{n-1}} |f_{n-1}|^2\pi_{n-1} + \sum_{n=0}^\infty |f_{n}|^2\pi_n + \sum_{n=0}^\infty \frac{p_n}{q_{n+1}} |f_{n+1}|^2\pi_{n+1} < \infty.
                \end{split}
            \end{equation*}
            \cb{TODO No puedo sacar de la sumatoria algo que depende de $n$, puedo justificarlo con que $f_{n-1}$ y $f_{n+1}$ tb es del espacio. O no. Preguntar a Payá.}

            \cb{REVIEW Pero que $\|Pf\|_\pi^2\leq \infty$ te dice que $Pf$ está en el espacio no que sea acotado el operador no?}
            \item $\|P\|\leq 1$: Sabemos que, al ser $P$ estocástica por filas, la norma infinito de $P$ es exactamente $1$, $\|P\|_\infty=1$. Por otro lado, por un resultado básico sabemos que el radio espectral de una matriz $A$ es menor o igual que cualquier otra norma matricial aplicada a $A$. Esto es, $\rho(P)\leq\|P\|_\infty = 1$. Como, $\rho(P)=\max\{|\lambda|:\lambda\text{ es un valor propio de }P\}$, necesariamente todos los valores propios de $P$ son, en valor absoluto, menores o iguales que $1$. 
            
            Tenemos entonces que $\|P f\|_\pi = \|\lambda f \|_\pi = |\lambda|\|f\|_\pi\leq \|f\|\pi$ para todo valor propio $\lambda$ de $P$. Como $\|P f\|_\pi \leq \|f\|_\pi$ y $\|P\|$ es la menor constante $K\geq 0$ tal que $\|Pf\|_\pi\leq K \|f\|_\pi$, necesariamente $\|P\|\leq 1$.
            \cb{REVIEW: La propiedad esa del radio espectral también vale para matrices infinitas?}

            \item Autoadjunto: Recordemos la relación de simetría \eqref{eq:ecs-simetria} ($\pi_n p_n = \pi_{n+1}q_{n+1}$). Entonces
            \begin{equation*}
                \begin{split}
                    \prodesc{Pf}{g}_\pi&=\sum_{n=0}^\infty(p_n f_{n+1} + r_n f_n + q_n f_{n-1})g_n\pi_n\\
                    &= \sum_{n=0}^\infty p_n f_{n+1}g_n\pi_n + \sum_{n=0}^\infty r_n f_{n}g_n\pi_n + \sum_{n=0}^\infty q_n f_{n-1}g_n\pi_n \\
                    &= \sum_{n=0}^\infty q_{n+1} f_{n+1}g_n\pi_{n+1} + \sum_{n=0}^\infty r_n f_{n}g_n\pi_n + \sum_{n=0}^\infty p_{n-1} f_{n-1}g_n\pi_{n-1} \\
                    &= \sum_{n=0}^\infty q_{n} f_{n}g_{n-1}\pi_{n} + \sum_{n=0}^\infty r_n f_{n}g_n\pi_n + \sum_{n=0}^\infty p_{n} f_{n}g_{n+1}\pi_{n} \\
                    &= \sum_{n=0}^\infty f_n(p_n g_{n+1} + r_n g_n+ q_n g_{n-1})\pi_n = \prodesc{f}{Pg}_\pi.
                \end{split}  
            \end{equation*}
            
            En la cuarta igualdad hemos modificado los índices de la primera y de la tercera sumatoria aprovechando el convenio de que $f_{-1}=g_{-1}=0$.

        \end{itemize}

    \end{proof}

    Con este lema nos aseguramos que el operador $P$ verifica las condiciones del teorema espectral \ref{th:espectral}, aunque para entender algo mejor qué consecuencias tiene aplicar este teorema primero estudiaremos en mayor profundidad el espacio $\ell^2_\pi(\N_0)$.

    Para $i\geq 0$, denotamos con $e^{(i)}\in\ell^2_\pi(\N_0)$ a las sucesiones tales que $(e^{(i)})_j = \delta_{ij}/\pi_i$, que expresada en forma de vector columna tendrían la forma
    $$
    e^{(i)}=(\overbrace{0,\dots,0}^{(i-1)},1/\pi_i,0,\dots)^T.
    $$
    Si aplicamos el operador $P$ a estos elementos particulares, obtenemos
    \begin{equation*}
        \begin{split}
            Pe^{(i)} &= (\overbrace{0,\dots,0}^{(i-2)}, p_{i-1}/\pi_i, r_i/\pi_i, q_{i+1}/\pi_i,0,\dots)^T \\
            &= (0,\dots,0, q_i/\pi_{i-1},  r_i/\pi_i, p_i/\pi_{i+1},0,\dots)^T \ \ \ \text{(por \eqref{eq:ecs-simetria})}\\
            &= q_i e^{(i-1)} + r_i e^{(i)} + p_i e^{(i+1)}.
        \end{split}
    \end{equation*}

    Hemos deducido la siguiente ecuación, que nos será útil próximamente en la demostración de la proposición \ref{prop:formula-e}:

    \begin{equation}
        \label{eq:Pen}
        Pe^{(n)} =  q_n e^{(n-1)} + r_n  e^{(n)} + p_n e^{(n+1)} \ \ \ (n\geq 1)
    \end{equation}

    Seguidamente introduciremos un resultado para el cual conviene recordar que, dado un polinomio cualquiera $\pi(x)= a_0 + a_1 x + \cdots + a_n x^n$ y una matriz cuadrada $A$, es posible evaluar el polinomio en $A$ sin más que considerar
    $$
    \pi(A) = a_0 I + a_1 A + \cdots + a_n A^n.
    $$
    Esta posibilidad nos ofrece una llamativa proposición.

    \begin{proposicion}
        \label{prop:formula-e}
        Para $n\geq 0$,
        \begin{equation}
            Q_n(P)e^{(0)} = e^{(n)}
        \end{equation}
    \end{proposicion}
    \begin{proof}
        Lo probaremos por inducción.
        \begin{itemize}
            \item Caso base $n=0$. Tenemos que $Q_0(x)=1$, luego $Q_0(P)=I$. Por tanto $Q_0(P) e^{(0)} = e^{(0)}$.
            \item Caso base $n=1$. En este caso, por la segunda ecuación de \eqref{eq:polinomios-pnm} y teniendo en cuenta que $Q_0(x)=1$,
            $$
            Q_1(x) = \frac{1}{p_0}(x-r_0).
            $$
            Por tanto, $Q_1(P) = \frac{1}{p_0}(P-r_0 I)$. Si multiplicamos por $e^{(0)}=(1,0,\dots)^T$ obtenemos:
            \begin{equation*}
                \begin{split}
                    Q_1(P)e^{(0)} &=  \frac{1}{p_0}(P-r_0 I) e^{(0)} \\
                    &=  \frac{1}{p_0} \begin{pmatrix}
                        0 & p_0 & 0  & \cdots & 0 \\
                        q_1 & r_1-r_0 & p_1 & \cdots & 0 \\
                        0 & q_2 & r_2-r_0 & \cdots & 0 \\
                        \vdots & \vdots & \vdots &\ddots & \vdots
                    \end{pmatrix} \begin{pmatrix}
                        1 \\ 0 \\ 0 \\ \vdots 
                    \end{pmatrix} = \begin{pmatrix}
                        0 \\ q_1/p_0 \\ 0 \\ \vdots
                    \end{pmatrix}= e^{(1)}
                \end{split}
            \end{equation*}
            \item Inducción. Supongamos que la fórmula es cierta para valores $\leq n$ y comprobemos si es cierta para $n+1$. Previamente, por la tercera ecuación de \eqref{eq:polinomios-pnm}, obtenemos que 
            $$
            Q_{n+1}(x) = \dfrac{1}{p_n}((x-r_n)Q_n(x)-q_n Q_{n-1}(x)),
            $$
            y si evaluamos matricialmente el polinomio en $P$ y multiplicamos por $e^{(0)}$:
            \begin{equation*}
                \begin{split}
                    Q_{n+1}(P)e^{(0)} &= \dfrac{1}{p_n}((P-r_n I)Q_n(P)e^{(0)}-q_n Q_{n-1}(P)e^{(0)}) \\
                    &= \dfrac{1}{p_n}((P-r_n I)e^{(n)}-q_n e^{(n-1)}) \ \  \ \text{(aplicando la hipótesis de inducción)} \\
                    &= \dfrac{1}{p_n}(Pe^{(n)} -r_n e^{(n)}-q_n e^{(n-1)}) \\
                    &= e^{(n+1)} \ \ \ \text{(por la igualdad \eqref{eq:Pen})}.
                \end{split}
            \end{equation*}
        \end{itemize}
    \end{proof}

    A nivel intuitivo, esta proposición nos dice que los elementos $e^{(n)}$ tal como los hemos definido son la primera columna de la matriz infinita que resulta al evaluar el $n$-ésimo polinomio $Q_n(x)$ definido en \eqref{eq:polinomios-pnm} matricialmente en la matriz $P$ definida en \eqref{eq:matriz-pnm}.

    Seguidamente, buscaremos expresar la probabilidad $P_{ij}^{(n)}$ en función de productos escalares a los que podamos aplicar el teorema espectral. Así, tenemos
    \begin{equation*}
        \begin{split}
            \prodesc{P^n e^{(j)}}{e^{(i)}}_\pi &= \prodesc{\left(\dfrac{1}{\pi_j}\left(P^n_{0,j},P^n_{1,j},\dots\right)\right)^T}{\left(0,0,\dots, \dfrac{1}{\pi_i},\dots \right)^T}_\pi \\
            &= \dfrac{1}{\pi_j} \sum_{k=0}^\infty P^n_{k,j} \dfrac{\delta_{ik}}{\pi_i}\pi_k = \dfrac{P^n_{ij}}{\pi_j}=\dfrac{P^{(n)}_{ij}}{\pi_j}.
        \end{split}
    \end{equation*}

    Y si despejamos $P^{(n)}_{ij}$, obtenemos
    \begin{equation*}
        P^{(n)}_{ij} = \pi_j \prodesc{P^n e^{(j)}}{e^{(i)}}_\pi,
    \end{equation*}
    que aplicando la proposición \ref{prop:formula-e} podemos escribir como
    \begin{equation*}
        P^{(n)}_{ij} = \pi_j \prodesc{P^n Q_j(P)e^{(0)}}{Q_{i}(P)e^{(0)}}_\pi.
    \end{equation*}
    Aplicando que $P$, y por tanto $P^k$ para $k=0,\dots,i$ es un operador autoadjunto, tenemos
    \begin{equation*}
        P^{(n)}_{ij} = \pi_j \prodesc{P^n Q_j(P)Q_{i}(P)e^{(0)}}{e^{(0)}}_\pi.
    \end{equation*}

    Atendiendo al último párrafo del apéndice \ref{appendix:teorema-espectral} y aplicando el teorema espectral \ref{th:espectral}, tomando $\mathcal{H}=\ell_\pi^2(\N_0)$, $f(x)=x^n Q_j(x)Q_i(x)$, $T=f(P)$, $x=y=e^{(0)}$, podemos afirmar que existe una medida $\psi=E_{e^{(0)},e^{(0)}}$, cuyo soporte está contenido en $[-\|P\|,\|P\|]\subseteq[-1,1]$, tal que 
    \begin{equation*}
        \int_{-1}^1 f(x)d\psi(x) = \prodesc{f(P)e^{(0)}}{e^{(0)}}_\pi,
    \end{equation*}
    es decir
    \begin{equation*}
        \int_{-1}^1 x^n Q_j(x)Q_i(x)d\psi(x) = \prodesc{P^n Q_j(P)Q_i(P)e^{(0)}}{e^{(0)}}_\pi = \dfrac{P_{ij}^{(n)}}{\pi_j}.
    \end{equation*}
    De donde deducimos la fórmula de representación
    \begin{equation*}
        P_{ij}^{(n)}=\pi_j \int_{-1}^1 x^n Q_j(x)Q_i(x)d\psi(x).
    \end{equation*}

    Resumimos esta deducción en el siguiente teorema.

    \begin{teorema}[Fórmula de Representación de Karlin-McGregor]
        Sea $\{X_n, n\geq 0\}$ un proceso de nacimiento y muerte discreto con matriz de transición $P$ definida como en \eqref{eq:matriz-pnm}. Entonces existe una única medida $\psi$, con soporte contenido en el intervalo $[-1,1]$ tal que para cualesquiera estados $i,j$ y cualquier instante $n$ se tiene que
        \begin{equation}
            \label{eq:formula-representacion}
            P_{ij}^{(n)}=\pi_j \int_{-1}^1 x^n Q_j(x)Q_i(x)d\psi(x).
        \end{equation}
    \end{teorema}
    \begin{proof}
        La existencia está probada, la unicidad puede probarse teniendo en cuenta que tomando $i=j=0$ se obtienen los momentos de $\psi$: $\int_{-1}^1 x^n d\psi(x)$. Estos momentos determinan unívocamente a la medida $\psi$. Consúltese \cite[Theorem 1]{random-walks}.
    \end{proof}

    \cb{REVIEW He puesto que la medida se denota con $\psi$ mientras que en el primer capítulo puse $\mu$, creo que convendría poner ambas iguales.}

    Como ya hemos adelantado anteriormente, de la medida $\psi$ que nos proporciona el teorema espectral obtendremos la ortogonalidad de los polinomios $Q_n(x)$ dados en \eqref{eq:polinomios-pnm}. Esta propiedad es, de hecho, una sencilla consecuencia de la fórmula de representación.

    \begin{corolario}
        Los polinomios $\{Q_n\}$ definidos en \eqref{eq:polinomios-pnm} son ortogonales respecto a la medida $\psi$.
    \end{corolario}
    \begin{proof}
        Si tomamos $n=0$ en la fórmula de representación \eqref{eq:formula-representacion} y recordamos que $P^{(0)}=I$, la matriz identidad, que en este caso sería infinita, obtenemos
        $$
        \int_{-1}^1 Q_j(x)Q_i(x)d\psi(x) = \frac{1}{\pi_j}\delta_{ij}.
        $$
    \end{proof}

    A la medida $\psi$ se le suele denominar \textbf{medida espectral} y a los polinomios $\{Q_n\}$ \textbf{polinomios de nacimiento y muerte}. 

    Hasta el momento, hemos probado que cualquier proceso de nacimiento y muerte admite una única medida espectral cuyo soporte está contenido en el intervalo $[-1,1]$ y tal que sus polinomios de nacimiento y muerte, definidos en \eqref{eq:polinomios-pnm}, forman una SPO. Además, las probabilidades de transición pueden ser calculadas utilizando integrales mediante la fórmula de representación de Karlin-McGregor \eqref{eq:formula-representacion}. Sin embargo, es posible también estudiar el problema inverso, es decir, a partir de una medida $\psi$ con soporte contenido en $[-1,1]$, conocer la existencia y/o unicidad de los posibles procesos de nacimiento y muerte para los cuales $\psi$ es su medida espectral. Por extensión no abordamos este aspecto en este trabajo, pero aconsejamos al lector interesado profundizar en \cite[Section 2.2]{Manuel} y \cite{random-walks}.

    \begin{ejemplo}[Proceso de nacimiento y muerte generado por los polinomios de Jacobi]
        Consideremos el proceso de nacimiento y muerte dado por los parámetros
        \begin{align*}
            p_n &= \dfrac{n+2\alpha+1}{2n+2\alpha+1} & r_n&= 0 & q_n = \dfrac{n}{2n+2\alpha+1},
        \end{align*}
        con $\alpha>-1$. Trataremos de calcular los polinomios de nacimiento y muerte, la medida espectral y la fórmula de representación. Comprobaremos que los polinomios de nacimiento y muerte $Q_n(x)$ son, en esencia, los polinomios de Jacobi presentados en la sección \ref{subsection:jacobi}, tomando $\alpha=\beta$.

        De acuerdo a las ecuaciones \eqref{eq:polinomios-pnm}, los polinomios $Q_n(x)$ del proceso obedecen a la relación de recurrencia:
        \begin{equation}
            \label{eq:recurrencia-ejemplo}
            x Q_n(x) = \dfrac{n+2\alpha+1}{2n+2\alpha+1} Q_{n+1} + \dfrac{n}{2n+2\alpha+1} Q_{n-1}.
        \end{equation}
        
        Los polinomios de Jacobi $P^{(\alpha,\beta)}_n(x)$, particularizando al caso $\alpha=\beta$, obedecen a la relación de recurrencia dada por (consúltese apéndice \ref{appendix:tablas}):

        $$
            (2n+2\alpha)_3\, x P^{(\alpha,\alpha)}_n(x) = 2(n+1)(n+2\alpha+1)(2n+2\alpha)P^{(\alpha,\alpha)}_{n+1}(x) + 2(n+\alpha)^2(2n+2\alpha+2)P^{(\alpha,\alpha)}_{n-1}(x)
        $$o equivalentemente \begin{equation}
            \label{eq:ecuaciones-jacobi-ejemplo}
            x P^{(\alpha,\alpha)}_n(x) = \dfrac{2(n+1)(n+2\alpha+1)}{(2n+2\alpha+1)(2n+2\alpha+2)} P^{(\alpha,\alpha)}_{n+1}(x) + \dfrac{n+\alpha}{2n+2\alpha+1}P^{(\alpha,\alpha)}_{n-1}(x)
        \end{equation}
        
        Consideramos $Q_n$ como la estandarización de los polinomios de Jacobi según:
        \begin{equation}
            \label{eq:polinomios-ejemplo}
            Q_n(x) = \dfrac{P^{(\alpha,\alpha)}_n(x)}{P^{(\alpha,\alpha)}_n(1)}.
        \end{equation}
        Si sustituimos en la ecuación \eqref{eq:recurrencia-ejemplo} teniendo en cuenta que $P^{(\alpha,\alpha)}_n(1)=\dfrac{(\alpha+1)_n}{n!}$ (véase \cite[Table 22.4]{abramowitz-stegun}) obtenemos
        $$
        x P^{(\alpha,\alpha)}_n(x) \dfrac{n!}{(\alpha+1)_n} = \dfrac{n+2\alpha+1}{2n+2\alpha+1} \dfrac{(n+1)!}{(\alpha+1)_{n+1}} P^{(\alpha,\alpha)}_{n+1}(x) + \dfrac{n}{2n+2\alpha+1} \dfrac{(n-1)!}{(\alpha+1)_{n-1}} P^{(\alpha,\alpha)}_{n-1}(x),
        $$
        que mediante sencillas simplificaciones podemos comprobar que, en efecto, es equivalente a \eqref{eq:ecuaciones-jacobi-ejemplo}.

        Por tanto, los polinomios de nacimiento y muerte de este proceso son los definidos en \eqref{eq:polinomios-ejemplo}. Tenemos entonces que los polinomios $\{Q_n(x)\}$ del proceso son ortogonales en el intervalo $[-1,1]$ y con respecto a la función peso
        $$
        \rho(x) = c\, (1-x^2)^\alpha,
        $$
        donde $c$ es una constante de estandarización.

        Por otro lado, mediante un sencillo cálculo tenemos
        \begin{equation}
            \begin{split}
                \pi_i &=\dfrac{p_0\cdots p_{i-1}}{q_1\cdots q_i} = \dfrac{\dfrac{1+2\alpha+1}{2+2\alpha+1}\cdot\dfrac{2+2\alpha+1}{4+2\alpha+1}\cdots \dfrac{i+2\alpha}{2(i-1)+2\alpha+1}}{\dfrac{1}{2+2\alpha+1}\cdot\dfrac{2}{4+2\alpha+1}\cdots \dfrac{i}{2i+2\alpha+1}} \\
                &= \dfrac{(2\alpha+2)_{i-1}(2i+2\alpha+1)}{i!}.
            \end{split}
        \end{equation}

        Finalmente, podemos aplicar la fórmula de representación \eqref{eq:formula-representacion} para obtener
        \begin{equation*}
            \begin{split}
                P_{ij}^{(n)}& = P[X_n=j|X_0=i] \\
                &= \pi_j \int_{-1}^1 x^n Q_i(x)Q_j(x)\rho(x)dx \\
                &= \dfrac{(2\alpha+2)_{j-1}(2j+2\alpha+1)}{j!} \int_{-1}^1 x^n \left[\dfrac{P^{(\alpha,\alpha)}_i(x)}{P^{(\alpha,\alpha)}_i(1)}\right]\left[\dfrac{P^{(\alpha,\alpha)}_j(x)}{P^{(\alpha,\alpha)}_j(1)}\right]\rho(x) dx \\
                &= \dfrac{(2\alpha+2)_{j-1}(2j+2\alpha+1)}{(j!)P^{(\alpha,\alpha)}_i(1)\, P^{(\alpha,\alpha)}_j(1)} \int_{-1}^1 x^n P^{(\alpha,\alpha)}_i(x) P^{(\alpha,\alpha)}_j(x)\rho(x) dx.
            \end{split}
        \end{equation*} 

    \end{ejemplo}

    \section{Análogos continuos de la fórmula de representación}

    Hasta el momento, en este capítulo y los anteriores hemos descrito la relación entre las cadenas de Markov, concretamente las cadenas de nacimiento y muerte, y los polinomios ortogonales. Esta relación se encuentra en la fórmula de representación de Samuel Karlin y James McGregor \eqref{eq:formula-representacion}, mediante la cual podemos calcular fácilmente probabilidades mediante integrales de manera sencilla teniendo en cuenta la ortogonalidad de los polinomios $Q_n(x)$ descritos en \eqref{eq:polinomios-pnm}.

    Sin embargo, en todo momento nos hemos restringido al caso estrictamente discreto: cadenas de Markov discretas, procesos de nacimiento y muerte discretos... que nos llevan a una versión discreta de la fórmula de representación. Sin embargo, existe una definición continua de las cadenas de Markov y de los procesos de nacimiento y muerte a partir de la cual se obtiene una expresión continua de la fórmula de representación de Karlin McGregor.

    En esta sección introduciremos superficialmente los análogos continuos de los procesos de nacimiento y muerte y la fórmula de representación de Karlin-McGregor. El siguiente texto está basado en las referencias \cite{Anderson}, \cite[Ch. 2, 3]{schoutens-2000}, \cite{differential-equations} y \cite[Sections 3.1, 3.2]{Manuel}, a las cuales remitimos al lector que desee profundizar.

    \subsection{Procesos de Markov continuos}

    Hasta el momento, hemos considerado el paso del tiempo como incrementos del valor de un parámetro $n=0,1,2,\dots$ que representa el instante $n$-ésimo. En el caso continuo, de forma general consideramos que un instante viene dado por un valor $t\in[0,+\infty)$. Por su parte, el espacio de estados $S$ no cambia, $S=\{0,1,\dots,N\}$ en el caso finito o $S=\N_0$ en caso de que exista una cantidad infinita de posibles estados. Así, fijado $t\in[0,+\infty)$, llamamos $X_t\in S$ a la variable aleatoria ``estado del sistema en el instante $t$'' y la colección de variables aleatorias $\{X_t, t\geq 0\}$ es un \textbf{proceso de Markov continuo}.
    
    En este caso, la propiedad de Markov viene dada como:
    \begin{equation}
        P[X_{t+s}=j|X_s=i, X_\tau, 0\leq \tau < s] = P[X_{t+s}=j|X_s=i], \ \ \ s,t>0.
    \end{equation}
    Es decir, el estado en el que se encuentre el proceso en un instante $t+s$ depende únicamente del estado en el que se encuentre en un instante anterior $t$, independientemente de lo que haya ocurrido antes de $t$. De nuevo, nos centraremos en procesos homogéneos, que son aquellos en los cuales las probabilidades únicamente dependen de la diferencia $t-s$, por lo que podemos eliminar la dependencia de $s$ y denotar
    \begin{equation}
        \label{eq:prob-trans-cont}
       p_{ij}(t) := P[X_{t}=j|X_0=i]  \ \ \ t>0.
    \end{equation}
    
    De esta forma, vemos que las probabilidades de transición se expresan en función del tiempo, de forma que lo que en el caso discreto era la matriz de transición $P$ dada en \eqref{eq:matriz-trans}, ahora es una función matricial $P(t)$:

    \begin{equation}
        \label{eq:matriz-trans-cont}
        P(t) = \begin{pmatrix}
            p_{00}(t) & p_{01}(t) & p_{02}(t)  & \cdots \\
            p_{10}(t) & p_{11}(t) & p_{12}(t)  & \cdots \\
            \vdots & \vdots & \vdots  & \ddots 
        \end{pmatrix}.
    \end{equation}

    \subsection{Procesos de nacimiento y muerte continuos}

    Particularizaremos al caso de procesos de nacimiento y muerte, en los que recordamos que su principal particularidad era la posibilidad de que en cualquier instante tan sólo era posible cambiar al siguiente estado, al anterior, o mantenerse en el mismo estado. Esta propiedad, llevada al modelo continuo, implica que si en un instante $t$ el sistema se sitúa en el estado $i$, únicamente podemos permitir que $p_{i,i-1}(t)$, $p_{i,i}(t)$ o $p_{i,i+1}(t)$ sean no nulas. Por tanto, la matriz \eqref{eq:matriz-trans-cont}, que extiende a la matriz \eqref{eq:matriz-pnm} al caso continuo, sería la siguiente:

    \begin{equation}
        \label{eq:matriz-pnm-cont}
        P(t) = \begin{pmatrix}
            p_{00}(t) & p_{01}(t) & 0 & 0  & \cdots \\
            p_{10}(t) & p_{11}(t) & p_{12}(t) & 0  & \cdots \\

            0 & p_{21}(t) & p_{22}(t) & p_{23}(t) &  \cdots \\
            \vdots & \ddots & \ddots & \ddots & \ddots 
        \end{pmatrix}.
    \end{equation}

    Además, asumiremos que las probabilidades \eqref{eq:prob-trans-cont} satisfacen las siguientes propiedades:

    \cb{TODO Revisar cuando he puesto que los estados empiezan en 0 y cuando en 1 que tengo un follón montado curioso}.

    \begin{enumerate}
        \item $p_{i,i+1}(h) = \lambda_i h + o(h)$ cuando $h\searrow 0$, $i\geq 0$.
        \item $p_{i,i-1}(h) = \mu_i h + o(h)$ cuando $h\searrow 0$, $i\geq 1$.
        \item $p_{i,i}(h) = 1 - (\lambda_i + \mu_i) h + o(h)$  cuando $h\searrow 0$, $i\geq 0$ asumiendo $\mu_0=0$ salvo que se indique lo contrario.
        \item $p_{ij}(0)=\delta_{ij}$.
        \item Asumimos que $\lambda_0>0$, $\lambda_i, \mu_i > 0$, $i\geq 1$.
    \end{enumerate}

    Los parámetros $\lambda_i, \mu_i$ son las llamadas \textbf{tasas de nacimiento y muerte}, respectivamente. En los puntos 1 y 2 afirmamos que, si el proceso empieza en el estado $i$, en un intervalo pequeño de tiempo las probabilidades de avanar o retroceder son aproximadamente proporcionales a la longitud del intervalo, siendo $\lambda_i$ y $\mu_i$ las constantes que marcan esta proporcionalidad. En el punto 3 asumimos que no existen otras posibles transiciones que avanzar, retroceder o mantenerse en el estado $i$.

    Las constantes $\{\mu_i\}, \{\lambda_i\}$ determinan por completo el comportamiento del proceso. Recogemos toda esta información en una matriz concreta.
    
    \begin{definicion}[Matriz generadora infinitesimal]
        Dado un proceso de nacimiento y muerte continuo $\{X_t, t\geq 0\}$ caracterizado por las constantes $\{\mu_i\}, \{\lambda_i\}$, definimos su \textbf{matriz generadora infinitesimal} como:
        \begin{equation}
            \label{eq:matrix-generadora}
            \mathcal A = \begin{pmatrix}
                -(\lambda_0+\mu_0) & \lambda_0 & 0 & 0 & \cdots \\
                \mu_1 & -(\lambda_1+\mu_1) & \lambda_1 & 0 & \cdots \\
                0 & \mu_2 & -(\lambda_2+\mu_2) & \lambda_2 & \cdots \\
                \vdots & \ddots & \ddots & \ddots & \ddots
            \end{pmatrix}.
        \end{equation}
    \end{definicion}

    Más información acerca de la génesis y la importancia de esta matriz en \cite[Section 2.1]{schoutens-2000}.

    De forma similar al caso de procesos de nacimiento y muerte discretos, podemos definir a partir de $\{\mu_i\}$ y $\{\lambda_i\}$ los coeficientes potenciales
    \begin{align}
        \label{eq:coeficientes-potenciales-cont}
        \pi_0&= 1 & \pi_n &= \dfrac{\lambda_0 \lambda_1 \cdots \lambda_{n-1}}{\mu_1 \mu_2 \cdots \mu_n} \ \ \ (n\geq 1).
    \end{align}
    Estos coeficientes verifican las ecuaciones de simetría
    \begin{equation}
        \label{eq:ecs-simetria-cont}
        \pi_n \lambda_n = \pi_{n+1}\mu_{n+1} \ \ \ (n\geq 0).
    \end{equation} 

    La matriz \eqref{eq:matrix-generadora} y los coeficientes \eqref{eq:coeficientes-potenciales-cont} jugarán un papel esencial en el análogo continuo de la fórmula de representación.

    \subsection{La fórmula de representación de Karlin y McGregor continua}

    De manera análoga a la sección \ref{section:formula-representacion}, presentaremos los llamados polinomios de nacimiento y muerte y la versión continua de la fórmula de representación.

    Para empezar, a partir de la matriz generadora infinitesimal \eqref{eq:matrix-generadora}, definimos una sucesión de polinomios $\{Q_n\}$ con $\deg(Q_n)=n$:
    \begin{equation}
        \label{eq:polinomios-pnm-cont}
        \begin{split}
            Q_{0}(x) &= 1 \\
            -xQ_0(x)&=\lambda_0 Q_1(x) -(\lambda_0 + \mu_0) Q_0(x), \\
            -x Q_n(x) &= \lambda_n Q_{n+1}(x) -(\lambda_n+\mu_n) Q_n(x) + \mu_n Q_{n-1}(x), \ \ \ (n\geq 1).
        \end{split}
    \end{equation}

    De forma vectorial, las ecuaciones pueden expresarse como $-x Q(x)=\mathcal{A} Q(x)$, donde $Q(x)=(Q_0(x), Q_1(x),\dots)^T$. Estos polinomios son los que nos permiten obtener la fórmula de representación de Karlin-McGregor continua:

    \begin{equation}
        \label{eq:formula-representacion-cont}
        P_{ij}(t)=\pi_j \int_{0}^\infty e^{-xt} Q_j(x)Q_i(x)d\psi(x), \ \ \ i,j\in\N_0, t\geq 0,
    \end{equation}
    donde $\psi$ es una medida cuyo soporte está contenido en el intervalo $[0,\infty)$ y para la cual los polinomios $\{Q_n\}$ definidos en \eqref{eq:polinomios-pnm-cont} forman una SPO.
    
    La demostración y los detalles de esta prueba pueden encontrarse en las referencias \cite{differential-equations} o \cite[Section 3.2]{Manuel}. Finalmente, para clarificar el uso de esta fórmula, presentamos un ejemplo.
    
\begin{ejemplo}[Procesos de nacimiento y muerte lineales]
    Sea el proceso de nacimiento y muerte dado por los parámetros
    \begin{align*}
        \lambda_n &=(n+\beta)\kappa & \mu_n = n\kappa, 
    \end{align*}
    donde $\beta,\kappa > 0$. Comprobaremos que los polinomios asociados a este proceso guardan una estrecha relación con los polinomios de Laguerre, introducidos en la sección \ref{subsec:Laguerre}.

    Con estos parámetros, los polinomios de nacimiento y muerte $Q_n(x)$ siguen la relación de recurrencia
    \begin{equation*}
        \begin{array}{c}
            -xQ_n(x) = \lambda_n Q_{n+1}(x) - (\lambda_n+\mu_n)Q_n(x) +\mu_n Q_{n-1}(x) \Leftrightarrow \\
            -x Q_n(x) = (n+\beta)\kappa Q_{n+1}(x) -(2n+\beta)\kappa Q_n(x) + n\kappa Q_{n-1}(x)
        \end{array}
    \end{equation*}
    Si dividimos la ecuación entre $\kappa$, obtenemos
    \begin{equation}
        \label{eq:ejemplo-laguerre1}
        -\frac{x}{\kappa} Q_n(x) = (n+\beta)Q_{n+1}(x) -(2n+\beta)Q_n(x) + n Q_{n-1}(x)
    \end{equation}
    

    Recordemos que los polinomios de Laguerre $L_n^\alpha(x)$ obedecen a una relación de recurrencia a tres términos dada por
    \begin{equation}
        \label{eq:ejemplo-laguerre2}
        -x L_n^\alpha = (n+1) L_{n+1}^\alpha(x) - (2n+\alpha+1)L_n^\alpha(x) + (n+\alpha)L_n^\alpha(x),
    \end{equation}
    con $\alpha > -1$. Tomamos entonces 
    \begin{equation}
        \label{eq:polinomiosQejemplo}
        Q_n(x) = \dfrac{n!}{(\beta)_n} L_n^{\beta-1}\left(\frac x \kappa\right),    
    \end{equation}
    
    Si tomamos $\alpha = \beta-1$, evaluamos (\ref{eq:ejemplo-laguerre2}) en $x/\kappa$ y multiplicamos ambos miembros de la igualdad por $\dfrac{n!}{(\beta)_n}$, podemos aplicar las siguientes igualdades en los términos en $n+1$ y en $n-1$:
    $$
    (n+1)\left[\dfrac{n!}{(\beta)_n}\right]L_{n+1}^{\beta-1} \left(\frac x \kappa\right) = (n+\beta)\dfrac{(n+1)!}{(\beta)_{n+1}} L_{n+1}^{\beta-1} \left(\frac x \kappa\right) = (n+\beta)Q_{n+1}(x),
    $$
    y
    $$
    (\beta+n-1)\left[\dfrac{n!}{(\beta)_n}\right]L_{n-1}^{\beta-1} \left(\frac x \kappa\right) = n\dfrac{(n-1)!}{(\beta)_{n-1}} L_{n-1}^{\beta-1} \left(\frac x \kappa\right) =n Q_{n-1}(x).
    $$
    
    Aplicando estas igualdades, tenemos que (\ref{eq:ejemplo-laguerre2}) es equivalente a (\ref{eq:ejemplo-laguerre1}).

    Por tanto, los polinomios asociados al proceso de nacimiento y muerte de este ejemplo son los presentados en (\ref{eq:polinomiosQejemplo}). Esto es, $Q_i(x)$ son ortogonales en $[0,+\infty)$ con respecto a la función peso 
    $$
    \rho(x) = ce^{-x/\kappa} x^{\beta-1},
    $$
    donde $c$ es una constante de estandarización.

    Podemos así calcular las probabilidades de transición aplicando la ecuación (TODO referencia) y teniendo en cuenta que
    $$
        \pi_i = \dfrac{\lambda_0\cdots \lambda_{i-1}}{\mu_1\cdots \mu_i} = \dfrac{\beta(\beta+1)\cdots (\beta+i-1)}{i!} = \dfrac{(\beta)_i}{i!}.
    $$
    Tenemos entonces

    \begin{equation*}
        \begin{split}
            P_{ij}(t) &= P[X_t = j/X_0=i] \\
            &= \pi_j\int_0^\infty e^{-xt}Q_i(x)Q_j(x)\rho(x)dx \\
            &= c\, \dfrac{(\beta)_j}{j!}\int_0^\infty e^{-xt}\left[\dfrac{i!}{(\beta)_i} L_i^{\beta-1}\left(\dfrac{x}{\kappa}\right)\right]\left[\dfrac{j!}{(\beta)_j} L_j^{\beta-1}\left(\dfrac{x}{\kappa}\right)\right]e^{-\frac x \kappa}x^{\beta-1}dx \\
            &= c\, \dfrac{(\beta)_i}{i!}\int_0^\infty e^{-x(t+\frac 1 \kappa)} x^{\beta-1} L_i^{\beta-1}\left(\frac x \kappa\right) L_j^{\beta-1}\left(\frac x \kappa\right) dx. 
        \end{split}
    \end{equation*}

\end{ejemplo}

En conclusión, una vez presentada la teoría de polinomios ortogonales y la de procesos y cadenas de Markov de manera independiente, aunque aparentemente no parecían guardar ninguna relación la una con la otra, hemos podido presentar la relación entre una y otra disciplina. Esta relación se sitúa en los procesos de nacimiento y muerte, en la que aprovechamos los tres coeficientes que caracterizan cada estado del sistema para crear lo que será una sucesión de polinomios ortogonales. Por su parte, la fórmula de representación de Karlin-McGregor permite encontrar un punto en común entre el cálculo de probabilidades y el de integrales, que aprovechando la ortogonalidad de los polinomios puede simplificarse considerablemente.

\cb{REVIEW No sé si poner que esto realmente facilita los cálculos de probabilidades. Aparentemente el cálculo de integrales no se ve más sencillo que el de probabilidades, pero teniendo en cuenta que aquí la cadena es infinita igual sí que compensa.}

